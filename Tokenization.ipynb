{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DAY 1: Tokenization\n",
        "* Tokenization is the text preprocessing step in NLP\n",
        "* Tokenization converts the paragraph in sentence or sentence in to words.\n",
        "* main terminologies related to tokenization:\n",
        "1. Corpus : paragraph\n",
        "2. Document : sentence\n",
        "3. vocabulary : Group of words\n",
        "4. words : unique tokens of each document/ paragraph\n",
        "* Tokenozation is imported from the nltk Library"
      ],
      "metadata": {
        "id": "b4MH89FFpA6t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5KMEswkFoUWL"
      },
      "outputs": [],
      "source": [
        "# corupus -> group of sentences / paragraph\n",
        "corpus = ''' I am a Student in Presidency university.\n",
        "I Love to learn NLP,ML,DL. I follow Krish naik's youtube channel\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing from the Library nltk\n",
        "from nltk import sent_tokenize,download,word_tokenize,wordpunct_tokenize,TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "q1dUXq3lpzUD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk requires punkt pakage\n",
        "download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQOO665XrHR7",
        "outputId": "3a34b029-a197-4fe9-d5d1-5103a5717f4f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# documents : Tokenizing the paragraph\n",
        "documents = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "cKPJGitUqEx4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing each documents / sentence\n",
        "for sentence in documents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUT3_rbRqNV3",
        "outputId": "afbfe933-2f9c-4210-bf96-611809f855e9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I am a Student in Presidency university.\n",
            "I Love to learn NLP,ML,DL.\n",
            "I follow Krish naik's youtube channel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing each words in the documents\n",
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw40q3rhrzMk",
        "outputId": "15614d88-749e-45f4-89bf-27c3b77e97fe"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'a', 'Student', 'in', 'Presidency', 'university', '.']\n",
            "['I', 'Love', 'to', 'learn', 'NLP', ',', 'ML', ',', 'DL', '.']\n",
            "['I', 'follow', 'Krish', 'naik', \"'s\", 'youtube', 'channel']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# punchuation marks also get separated\n",
        "for sentence in documents:\n",
        "  print(wordpunct_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLLxwYk7sTFF",
        "outputId": "39ff9217-3d6b-4151-9156-b331a1a4f348"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'a', 'Student', 'in', 'Presidency', 'university', '.']\n",
            "['I', 'Love', 'to', 'learn', 'NLP', ',', 'ML', ',', 'DL', '.']\n",
            "['I', 'follow', 'Krish', 'naik', \"'\", 's', 'youtube', 'channel']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tree bank word tokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mYZnnATsX_2",
        "outputId": "eeb6ed0b-20a0-4c1e-d21e-3de99a80337a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'am',\n",
              " 'a',\n",
              " 'Student',\n",
              " 'in',\n",
              " 'Presidency',\n",
              " 'university.',\n",
              " 'I',\n",
              " 'Love',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'NLP',\n",
              " ',',\n",
              " 'ML',\n",
              " ',',\n",
              " 'DL.',\n",
              " 'I',\n",
              " 'follow',\n",
              " 'Krish',\n",
              " 'naik',\n",
              " \"'s\",\n",
              " 'youtube',\n",
              " 'channel']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}